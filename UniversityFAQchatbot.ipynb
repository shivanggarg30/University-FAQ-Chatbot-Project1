{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oXaIkIaw94M",
        "outputId": "213bf042-8808-46cf-d694-37f724e51c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas nltk scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Download NLTK data (only need to do this once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdep7aZA09U4",
        "outputId": "fb007481-b1d9-4a59-9e18-8606bd801153"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Applies preprocessing (tokenization, stopword removal, lemmatization).\"\"\"\n",
        "    # 1. Tokenization and lowercasing\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "\n",
        "    # 2. Stopword removal and lemmatization\n",
        "    processed_tokens = [\n",
        "        lemmatizer.lemmatize(token) for token in tokens\n",
        "        if token.isalnum() and token not in stop_words\n",
        "    ]\n",
        "\n",
        "    return \" \".join(processed_tokens)"
      ],
      "metadata": {
        "id": "7fGTD_Dy2Rbk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the self-created CSV dataset\n",
        "try:\n",
        "    df = pd.read_csv('faq_dataset.csv')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: faq_dataset.csv not found.\")\n",
        "    # Handle error appropriately\n",
        "    exit()\n",
        "\n",
        "# Ensure columns are named correctly\n",
        "if 'Question' not in df.columns or 'Answer' not in df.columns:\n",
        "    print(\"Error: CSV must have 'Question' and 'Answer' columns.\")\n",
        "    exit()\n",
        "\n",
        "# Preprocess all questions in the dataset\n",
        "df['processed_question'] = df['Question'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "s43RMDB52R6e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Create the TF-IDF matrix for the dataset questions\n",
        "question_tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_question'])"
      ],
      "metadata": {
        "id": "MptRKhDx3Led"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_answer(user_query):\n",
        "    \"\"\"Finds and returns the best answer from the dataset.\"\"\"\n",
        "\n",
        "    # 1. Preprocess user input query\n",
        "    processed_query = preprocess_text(user_query)\n",
        "\n",
        "    # 2. Vectorize the user query using the *same* vectorizer\n",
        "    query_tfidf_vector = tfidf_vectorizer.transform([processed_query])\n",
        "\n",
        "    # 3. Compute cosine similarity [cite: 8, 26]\n",
        "    similarities = cosine_similarity(query_tfidf_vector, question_tfidf_matrix)\n",
        "\n",
        "    # 4. Find the index of the most similar question\n",
        "    best_match_index = np.argmax(similarities)\n",
        "\n",
        "    # Get the similarity score of the best match\n",
        "    best_match_score = similarities[0, best_match_index]\n",
        "\n",
        "    # 5. Return the best matching answer\n",
        "    # You can set a threshold to handle irrelevant questions\n",
        "    if best_match_score > 0.2: # Threshold (tune as needed)\n",
        "        return df['Answer'].iloc[best_match_index]\n",
        "    else:\n",
        "        return \"I'm sorry, I don't have an answer for that. Please try rephrasing.\""
      ],
      "metadata": {
        "id": "4-hyeNMC3RPY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Comment out the original interactive loop ---\n",
        "\n",
        "# print(\"University FAQ Chatbot: Ask me anything! (Type 'exit' to quit)\")\n",
        "# while True:\n",
        "#     user_input = input(\"You: \")\n",
        "#     if user_input.lower() == 'exit':\n",
        "#         break\n",
        "#\n",
        "#     answer = get_best_answer(user_input)\n",
        "#     print(f\"Bot: {answer}\")\n",
        "\n",
        "\n",
        "# --- 2. Add your test questions directly here ---\n",
        "\n",
        "print(\"--- Chatbot Test Run ---\")\n",
        "\n",
        "# Test Question 1\n",
        "query1 = \"How much is the admission fee?\"\n",
        "answer1 = get_best_answer(query1)\n",
        "print(f\"Q: {query1}\")\n",
        "print(f\"A: {answer1}\\n\") # \\n adds a new line for spacing\n",
        "\n",
        "# Test Question 2\n",
        "query2 = \"When do exams start?\"\n",
        "answer2 = get_best_answer(query2)\n",
        "print(f\"Q: {query2}\")\n",
        "print(f\"A: {answer2}\\n\")\n",
        "\n",
        "# Test Question 3\n",
        "query3 = \"What is the hostel cost?\"\n",
        "answer3 = get_best_answer(query3)\n",
        "print(f\"Q: {query3}\")\n",
        "print(f\"A: {answer3}\\n\")\n",
        "\n",
        "# Test Question 4 (An unknown question)\n",
        "query4 = \"What is the menu for the cafeteria?\"\n",
        "answer4 = get_best_answer(query4)\n",
        "print(f\"Q: {query4}\")\n",
        "print(f\"A: {answer4}\\n\")\n",
        "\n",
        "print(\"--- End of Test Run ---\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bczURIVu3tJw",
        "outputId": "1355c208-c6d4-42f2-c7b3-4fcc9080037d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chatbot Test Run ---\n",
            "Q: How much is the admission fee?\n",
            "A: Admission fee is ₹5000.\n",
            "\n",
            "Q: When do exams start?\n",
            "A: Exams will begin in December as per the academic calendar.\n",
            "\n",
            "Q: What is the hostel cost?\n",
            "A: The annual hostel fee is ₹1,20,000.\n",
            "\n",
            "Q: What is the menu for the cafeteria?\n",
            "A: I'm sorry, I don't have an answer for that. Please try rephrasing.\n",
            "\n",
            "--- End of Test Run ---\n"
          ]
        }
      ]
    }
  ]
}